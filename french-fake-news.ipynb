{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10327953,"sourceType":"datasetVersion","datasetId":6394877}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom transformers import CamembertTokenizer, CamembertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nimport torch\nfrom torch.utils.data import Dataset\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import SMOTE\n\n# Disable wandb tracking if not needed\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n\n# Get data\ntrain_data = pd.read_csv('../input/french-climate-change-fake-news/train.csv')\ntest_data = pd.read_csv('../input/french-climate-change-fake-news/test.csv')\n\n# Check data\nprint(\"Train data:\")\nprint(train_data.head())\nprint(\"\\nTest data:\")\nprint(test_data.head())\n\n# Split data\nX_train = train_data['Text']\ny_train = train_data['Label']\n\n# Map string labels to integers\nlabel_mapping = {label: idx for idx, label in enumerate(y_train.unique())}\ny_train = y_train.map(label_mapping)\n\n# Ensure test dataset labels are mapped too (if applicable, or create placeholder)\nif 'Label' in test_data.columns:\n    test_data['Label'] = test_data['Label'].map(label_mapping)\n\n# Perform Train-Test Split\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    X_train, y_train, test_size=0.2, random_state=42\n)\n\n# Addressing class imbalance by oversampling minority classes in the training set\nfrom collections import Counter\nfrom sklearn.utils import resample\n\n# Combine text and labels for oversampling\ntrain_data_combined = pd.DataFrame({'Text': X_train_split, 'Label': y_train_split})\n\n# Separate majority and minority classes\nclass_counts = Counter(y_train_split)\nmajority_class = max(class_counts, key=class_counts.get)\nminority_classes = [cls for cls in class_counts if cls != majority_class]\n\n# Oversample minority classes\noversampled_data = [train_data_combined[train_data_combined['Label'] == majority_class]]\nfor cls in minority_classes:\n    class_data = train_data_combined[train_data_combined['Label'] == cls]\n    oversampled_class_data = resample(class_data, \n                                      replace=True, \n                                      n_samples=class_counts[majority_class], \n                                      random_state=42)\n    oversampled_data.append(oversampled_class_data)\n\n# Combine oversampled data\noversampled_train_data = pd.concat(oversampled_data)\nX_train_resampled = oversampled_train_data['Text']\ny_train_resampled = oversampled_train_data['Label']\n\n# Custom Dataset for BERT\nclass NewsDataset(Dataset):\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = self.texts[idx]\n        label = self.labels[idx]\n        encoding = self.tokenizer(\n            text,\n            max_length=self.max_len,\n            padding=\"max_length\",\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        return {\n            \"input_ids\": encoding[\"input_ids\"].flatten(),\n            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n            \"labels\": torch.tensor(label, dtype=torch.long),\n        }\n\n# Load tokenizer and model\ntokenizer = CamembertTokenizer.from_pretrained(\"camembert-base\")\nmodel = CamembertForSequenceClassification.from_pretrained(\n    \"camembert-base\", num_labels=len(np.unique(y_train))\n)\n\n# Prepare data\nX_train_split, X_val, y_train_split, y_val = train_test_split(\n    X_train_resampled, y_train_resampled, test_size=0.2, random_state=42\n)\ntrain_dataset = NewsDataset(X_train_split.tolist(), y_train_split.tolist(), tokenizer, max_len=512)\nval_dataset = NewsDataset(X_val.tolist(), y_val.tolist(), tokenizer, max_len=512)\n\n# Training arguments\ntraining_args = TrainingArguments(\n    output_dir=\"./results\",\n    eval_strategy=\"epoch\",  # Evaluation happens at the end of each epoch\n    save_strategy=\"epoch\",  # Save the model at the end of each epoch\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=5,\n    weight_decay=0.01,\n    logging_dir=\"./logs\",\n    logging_steps=10,\n    save_total_limit=2,\n    report_to=[\"none\"],  # Disables wandb or other trackers\n    load_best_model_at_end=True,  # Load the best model at the end\n    metric_for_best_model=\"eval_loss\"  # Use evaluation loss to determine the best model\n)\n\n# Trainer\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    )\n\n# Train model\ntrainer.train()\n\n# Load the best model\nbest_model = trainer.model\n\n# Save the best model\ntorch.save({\n    'model_state_dict': best_model.state_dict(),\n    'tokenizer': tokenizer,\n    'label_mapping': label_mapping  # Save the label mapping for later use\n}, \"./best_model.pt\")\nprint(\"Best model saved: ./best_model.pt\")\n\n# Evaluate and print metrics for the best model\nval_results = trainer.evaluate()\nprint(\"Best Model Validation Results:\", val_results)\n\n# Generate detailed classification report using the best model\nval_predictions = trainer.predict(val_dataset)\ny_val_pred = np.argmax(val_predictions.predictions, axis=1)\nprint(\"\\nClassification Report:\\n\")\nprint(classification_report(y_val.tolist(), y_val_pred))\n\n# Confusion Matrix\nconf_matrix = confusion_matrix(y_val, y_val_pred, normalize='true')\nlabels = list(label_mapping.keys())\nplt.figure(figsize=(8, 6))\nsns.heatmap(conf_matrix, annot=True, fmt='.2f', cmap='Blues', xticklabels=labels, yticklabels=labels)\nplt.title('Normalized Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.title('Confusion Matrix')\nplt.xlabel('Predicted Labels')\nplt.ylabel('True Labels')\nplt.show()\n\n# Predict test data using the best model\ntest_dataset = NewsDataset(test_data[\"Text\"].tolist(), [0] * len(test_data), tokenizer, max_len=512)\ntest_predictions = trainer.predict(test_dataset)\ntest_data[\"Label\"] = pd.Series(np.argmax(test_predictions.predictions, axis=1)).map({v: k for k, v in label_mapping.items()})\n\n# Save test file with predictions\ntest_data.to_csv(\"test_with_predictions.csv\", index=False)","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null}]}